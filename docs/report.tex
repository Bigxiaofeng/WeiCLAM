\documentclass[12pt,fleqn]{article}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{vkCourseML}
\hypersetup{unicode=true}
\usepackage{xcolor}
%\usepackage[a4paper]{geometry}
\usepackage{cases}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{pdfpages}
\usepackage[cp1251]{inputenc}

\usepackage{url}
\usepackage{hyperref}
%\usepackage{caption}
%\usepackage{subcaption}

\interfootnotelinepenalty=10000
\bibliographystyle{unsrt} 

\newcommand{\thickhline}{%
	\noalign {\ifnum 0=`}\fi \hrule height 1pt
	\futurelet \reserved@a \@xhline
}



\newcommand\note[1]{ \textcolor{red}{ -| #1 |- } }


\def\XX{\mathbb{X}}
\def\PP{\mathbb{P}}
\def\FF{\mathcal{F}}
\def\EE{\mathbb{E}}
\def\NN{\mathcal{N}}
\newenvironment{esSolution}%
    {\begingroup\par\noindent{\bf Решение.}}%
    {\par\hfill$\scriptstyle\blacksquare$\medskip\endgroup}

%\hypersetup{linkcolor = Brown}

\begin{document}

\begin{titlepage}
\begin{center}

    \includegraphics[width=30mm]{./imgs/logo_hse_2.eps}

    \bigskip
    Федеральное государственное автономное образовательное учреждение\\
     высшего образования \\
    «Национальный исследовательский университет «Высшая школа экономики» \\[5mm]
    Факультет компьютерных наук\\
    Магистерская программа математических методов оптимизации и стохастики\\[30mm]

    \textsf{\large
      Курсовая работа \\[10mm]
      \textbf{
        Выделение пересекающихся сообществ \\
        во взвешенных графах\\
        \note{ЧЕРНОВИК}
      }
    }\\[30mm]

    \begin{flushright}
      \parbox{0.5\textwidth}{
        \raggedleft
        \textbf{Выполнил:}\\
        студент группы м15МОС\\
        Славнов Константин Анатольевич\\[5mm]
        \textbf{Научный руководитель:}\\
        к.ф.-м.н.\\
        Панов Максим Евгеньевич
      }
    \end{flushright}

    \vspace{\fill}
    Москва, 2016
\end{center}
\end{titlepage}

\newpage
\tableofcontents

\newpage

\section{Введение}
\note{Общие слова про тему --- выделение пересекающихся сообществ.\\
Актуальность --- зачем выделение сообществ, для каких задач надо.\\
Зачем пересекающиеся, про новизну анализа взвешенных графов. \\
Про ключевые результаты работы.}\\[16px]

В данной работе будет рассмотрена задача выделения сообществ --- группы вершин в графе, плотно связанных между собой, но не с остальным графом. На текущий момент известно множество подходов и методов для выделения непересекающихся сообществ \cite{Fortunato10}. Гораздо меньше внимания уделено случаю пересекающихся групп. В данной работе будет предложен новый метод решения задачи в случае взвешенных графов с пересекающимися группами вершин. Метод основан на алгоритме BigClam \cite{yang2013overlapping}, который разработан для случая невзвешенных графов. Можно сказать, что новый метод является обобщением модели BigClam на взвешенный случай. 

Работа начинается с постановки задачи и подробного описания метода BigClam. Особое внимание уделено методу инициализации. Будет показано, как небольшими усилиями можно улучшить предложенный метод инициализации, что ускоряет и немного уточняет итоговый результат. 

Далее речь пойдет о подходах обобщения метода на случай взвешенного графа. 
В начале рассмотрено самое простое и интуитивное обобщение, после чего предложена усложненная модель.
Заканчивается работа экспериментами на модельных данных и сравнением с другими методами. 

\note{Добавить выводы в общих словах, когда они будут сформулированы.}

\section{Постановка задачи}
\note{Описание общего подхода и принципа.} \\

Общий метод базируется на следующем очевидном наблюдении, что чем в большее количество сообществ входят одновременно две вершины, тем больше вероятность, что они будут соединены ребром, что подтверждается на реальных данных \cite{yang2013overlapping}. Наша модель должна учитывать этот факт.

Представим, что каждая вершина графа $v$ взаимодействует с сообществом $A$ с некоторой силой $F_{vA}$. Нулевая сила означает отсутствие взаимодействия. Такую модель можно представить как двусвязный граф, из вершин исходного графа в первой компоненте и вершин-сообществ во второй (рис. \ref{fig:AGM}). Отметим, что подобная концепция позволяет отразить не только идею пересекающихся сообществ, но и вложенных.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.5\textwidth]{imgs/BigCLAM_model.png}
	\caption{Двусвязный граф модели BigClam. Сверху вершины-сообщества, снизу вершины исходного графа. Вершины и сообщества взаимодействуют с неотрицательной силой $F_{vA}$. Ребра, которые соответствуют нулевым весам опущены для большей наглядности.}
	\label{fig:AGM}
\end{figure}

Теперь можно определить силу взаимодействия $X_{uv}$ между вершинами $u$ и $v$, которая будет определять вероятность появления ребра между вершинами: $X_{uv} = F_{u} \cdot F_{v}^T$, где $F_{u}$ --- вектор-строка, составленная из $F_{uA}$ --- сил взаимодействия вершин с сообществами графа.

Таким образом, получено желаемое свойство: чем больше общих сообществ разделяют вершины, тем сильнее они связаны. 
Определим вероятность появления ребра $(u,v)$ как $p(u, v) = 1 - \exp ( - X_{uv})$. 
Т.е. чем сильнее связаны вершины, тем вероятнее появление ребра между ними. 
Таким образом получена вероятностная модель. Предполагается, что наблюдаемый нами граф сгенерирован измено из нее.

Итак, кратко опишем оригинальный метод, его основные предположения.

\subsection{Cluster Affiliation Model for Big Networks (BigClam)}
\note{Постановка задачи, \\
предположения, \\
вывод формул, \\
схема оптимизации, \\
примерно как в ноутбуке Math Models. \\
Про AGM модель и ее релаксацию.\\
Рассказ про NMF с не квадратичной функцией потерь.\\
}

Опишем предположения полученной вероятностной модели формально. 

\paragraph{Основные обозначения.} Введем основные обозначения, которые будут использоваться на протяжении всей работы.\bigskip

\note{толстые линии}\\
\begin{tabular}{ p{7.5em}   p{29em} }
	\hline
	Обозначение & Описание \\
	\hline
	$G = (V,E) $ & граф \\                     
	$A \in \mathbb{R}_{+}^{N\times N}$  & матрица смежности   \\[5px]
	$F \in \mathbb{R}_{+}^{N\times K}$  & матрица силы принадлежности к сообществам			\\[5px]
	$K$    								& количество сообществ \\[5px]
	$C$    								& множество сообществ  \\[5px]
	$p(u,v)$							& вероятность появления ребра $(u,v)$ \\[5px]
	$p((u,v) \mid c)$ 					& вероятность появления ребра $(u,v)$ при условии, что $u$ и $v$ принадлежат сообществу $c$ \\[5px]
	
	$l(F)$								& логарифм функции правдоподобия \\[5px]
	$\mathcal{N}(u)$ 					& 1-окрестность вершины $u$ (cоседи вершины) \\[5px]
	$w_{uv}$							& вес ребра $(u,v)$ для взвешенного графа \\[5px]
	\hline
\end{tabular}
\bigskip

\paragraph{Предположения.} 
\begin{enumerate}
	\item Каждая вершина $v\in V$ относится к сообществу $c \in C$ с некоторой силой
	$$F_{vc} \ge 0.$$
	\item Вероятность появления ребра $(u,v)$, при условии, что вершины $u,v$ находятся в одном сообществе $c$ определяется по формуле 
	$$P((u,v) \mid c)=1 - \exp(-F_{uc}\cdot F_{vc}).$$
	\item Каждое сообщество $c$ генерирует ребра независимо друг от друга, а значит, что вероятность появления ребра можно посчитать по формуле полной вероятности. Получим
	
	$$P(u,v)=1 - \exp(-\sum_{c\in C} F_{uc} F_{vc}) = 1 - \exp( - F_{u} F_{v}^T),$$
	
	$$F = \{F_u\} = \{F_{uc}\} \in \mathbb{R}^{N \times K}. $$
\end{enumerate}

\paragraph{Вероятностная интерпретация.} 
Предложенная модель имеет простую вероятностную интерпретацию. 
Предположим, что существуют скрытые случайные переменные $X_{uv}$, которые определяют силу взаимодействия вершин, а ребро появляется если  $X_{uv} > 0$.
Каждое сообщество графа дает свой независимый вклад $X_{uv}^{(c)}$ в $X_{uv}$.
Предположим, что $X_{uv}^{(c)} \sim \mathrm{Pois}(F_{uc} \cdot F_{vc})$, где $F_{vc}\ge 0$ сила взаимодействия вершины $v$ и сообщества $c$. 
Значит, что 
$$X_{uv} \sim \mathrm{Pois}(\sum_{c} F_{uc} \cdot F_{vc}) = \mathrm{Pois}(F_{u} \cdot F_{v}^T).$$
Вероятность появления ребра равна 
$$p(u,v) = \mathbb{P}(X_{uv} > 0) = 1 - \exp( - F_{u} F_{v}^T),$$
что соответствует формулам, полученным выше.


\paragraph{Метод и Схема оптимизации}

Для восстановления матрицы $F$ предлагается использовать метод максимизации правдоподобия.
Из приведенных выше формул не сложно вывести, что правдоподобие $l(F)$ определяется как

\begin{align*}
	l(F) & = \log(\mathbb{P}(A\mid F))\\
	& = \sum_{(u,v)\in E} \log(1 - \exp( - F_{u} F_{v}^T)) - \sum_{(u,v) \notin E} F_u F_v^T.
\end{align*}

$$ \hat{F} = \arg \min_{F \ge 0} D(A, f(FF^T)),$$

$$\text{гдe}\quad D = -l(F), \quad f(x) = 1-\exp(-x).$$

Для оптимизации используется блочный координатный спуск с методом проекции градиента на каждом шаге.
Фиксируем $F_v$, оптимизируем по $F_u$, $u \ne v$. Задача становится выпуклой.

$$\forall u: \quad \arg\max_{F_u \ge 0} l(F_u), $$
$$l(F_u) = \sum_{v \in \mathcal{N}(u)} \log(1-\exp(-F_u F_v^T)) - \sum_{v \notin \mathcal{N}(u)} F_u F_v^T, $$
где $\mathcal{N}(u)$ — соседи вершины u.
$$\nabla l(F_u) = \sum_{v \in \mathcal{N}(u)} F_u \dfrac{\exp(-F_u F_v^T)}{1-\exp(-F_u F_v^T)} - \sum_{v \notin \mathcal{N}(u)} F_v^T. $$
Основная сложность формулы (линейная по размеру графа) во втором слагаемом. Заметим, что 
$$\sum_{v \notin \mathcal{N}(u)} F_v^T = \sum_v{F_v} - F_u - \sum_{v\in \mathcal{N}(u)} F_v.$$ 

Получаем сложность одной итерации $O(\mathcal{N}(u))$. 
В этом значимое отличие рассматриваемого метода от остальных. 
Такая сложность позволяет обсчитывать графы с количеством вершин до $10^5$ за приемлемое время.

Для подбора градиентного шага используется backtracking line search.

\paragraph{Связь с матричными разложениями}
Подобная постановка задачи, позволяет рассмотреть задачу выделения пересекающися сообществ, как задачу неотрицательного матричного разложения с общим функционалом.
То есть необходимо найти такую низкоранговую матрицу $F\in \mathbb{R}_{+}^{N \times K}$, что она наилучшим образом приближает матрицу $A$ в смысле некоторого функционала $D$:

$$ F = \argmin_{F\ge 0} D(A, f(F F^T)). $$

В данном случае $D = - l(F)$ --- значение правдоподобия, а $f(x) = 1 - \exp(-x)$ --- функция, которая преобразует силы взаимодействия вершин в вероятности появления ребра (link function).
Как показывают эксперименты такая постановка оправдана, т.к. $l_2$-норма плохо подходит для восстановления бинарных матриц \cite{yang2013overlapping}.\note{Возможно, другая ссылка}

Такое наблюдение позволяет смотреть на задачу выделения пересекающихся сообществ с более общих формулировок.

\paragraph{Восстановление структуры сообществ}
После того, как метод оптимизации сошелся к некоторому значению матрицы $F$, осталось предложить некоторый переход к сообществам.
Для того, чтобы восстановить исходную структуру сообществ $C$ сравним значение матрицы $F$ с порогом $\delta$, который определим далее. Если $F_{vc} > \delta$, то $v \in c$.

Обозначим за $\varepsilon$ вероятность появления ребра в графе (если бы все ребра появлялись равномерно): $\varepsilon = \dfrac{2|V|}{|E|\cdot (|E|-1)}$. 
Возьмем $\delta$ так, чтобы две вершины принадлежали одному сообществу, если модельная вероятность появления ребра между ними выше чем $\varepsilon$:
$$\varepsilon \le 1-\exp(-\delta^2).$$
А значит
$$\delta = \sqrt{-\log(1-\varepsilon)}. $$

\paragraph{ $\varepsilon$-сообщество}
Предложенная модель имеет недостаток. 
Если две вершины не разделяют хотя бы одного общего сообщества, между ними не может быть ребра.  
Очевидно, что в настоящих сетях такого свойства нет.
Поэтому введем так называемое $\varepsilon$-сообщество.
Предполагаем, что все вершины относятся к единому $\varepsilon$-сообществу с малой силой $\delta$, определенной выше.
То есть дополнительно предполагается, что ребро могло возникнуть случайно с вероятностью, которая равна доле существующих ребрер в графе:

$$ p((u,v)\mid \varepsilon) = \dfrac{2|V|}{|E| \cdot \left( |E| - 1 \right)}.$$

\section{Инициализация}
\note{Про Conductance и инициализацию в BigClam.\\
	про ее не совершенство. Идеи по улучшению. Раз, два, три.
	Тестирование на модельных данных. Тестирование на реальных данных.\\
	Выводы.}

\subsection{Оригинальный подход}

Так как задача не является выпуклой большое внимание необходимо уделить методам инициализации.
В данном методе используется подход, описанный в статье \cite{gleich2011neighborhoods}.

Введем метрику на подмножестве множества вершин $ S \subset V $.

$$\phi(S) = \dfrac{\mathrm{cut}(S)}{\min(\mathrm{vol}(S), \mathrm{vol}( \bar S))}, \text{где}$$
$$\mathrm{cut}(S) = \mathrm{cut}(S, \bar S)=\sum_{\substack{(v,u)\in E\\ v \in S \, u \in \bar S}} a_{vu},$$
$$\mathrm{vol}(S) =\sum_{(v,u)\in S} a_{vu}.$$

Величина $\phi(S)$ называется проводимостью (Conductance) и очень похоже на взвешенный разрез.
Утверждается, что эго-графы (вершина с ее 1-окрестностью), которые достигают локального значения функционала $\phi(S)$ являются хорошими сообществами и могут использоваться в качестве инициализации для других методов. 
Локальность понимается в смысле графа. 
Значение проводимости эго-графа любой соседней вершины должно быть больше, чем в данной. 

В качестве инициализации предлагается выбирать необходимое количество эго-графов, которые достигают локального максимума, упорядоченных по возрастанию.
Т.е. для $S_1, \dots, S_K$, $i \in V $

$$ 
F_{ij}=	
		\begin{cases} 	1,  \text{если } i \in S_j;\\
						0,  \text{иначе. } 
		\end{cases}
$$

\paragraph{Недостатки.}
Опишем минусы такого подхода. 
Матрица $F$ получается детерминирована, а значит метод бессмысленно перезапускать для улучшения результата.
$F$ состоит всего из двух значений 0 и 1, при чем подавляющее большинство нули. Ноль является плохой точкой для старта, т.к. $F=0$ --- точка локального минимума. 
То есть модель потенциально может сойтись к плохому значению функционала.
Помимо этого, часто так получается, что 2 или большее количество значений среди $S_1, \dots, S_K$ соответствуют одному сообществу. 
Продемонстрируем последний недостаток на следующем модельном примере.
Рассмотрим матрицу $F$ как на рис. \ref{fig:model_ex}. Сгенерируем из нее модельный граф согласно модели BigClam. И найдем 3 вершины, эго-графы которых будут образовывать начальное приближение матрицы $F$. На рис. \ref{fig:model_ex_graph} изображен семплированный граф и слева 3 найденные вершины отмечены красным цветом.

\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{imgs/model_example.png}
	\caption{Модельный пример графа из трех пересекающихся сообществ. Всего в графе 140 вершин. Описания изображений слева направо: 1) Матрица $F$ размера 3 на 140; 2) Матрица $1-\exp(FF^T)$ --- определяет вероятность появления ребра согласно модели BigClam; 3) Случайная семплированная матрица смежности.}
	\label{fig:model_ex}
\end{figure}


\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{imgs/model_example_graph_good_init.png}
	\caption{Граф, сгенерированный из матрицы F на рис. \ref{fig:model_ex}. Красным цветом обозначены вершины, эго-графы которых образуют начальное приближение матрицы F. \textbf{Слева:} оригинальный метод. 3 вершины с минимальной проводимостью, которые достигают локального минимума в окрестности вершины. Видно, что две вершины попали в один кластер, что портит начальное приближение. \textbf{Справа:} новый метод. 3 вершины с минимальной пеналированной проводимостью, которые достигают локального минимума в окрестности вершины. Видно, что все 3 вершины лежат в своих кластерах, что улучшает качество инициализации.}
	\label{fig:model_ex_graph}
\end{figure}

Видно, что 2 вершины являются представителями одного и того же сообщества. 
И это не единичный случай, такая ситуация часто встречалась и на реальных графах. 
Находились такие $i,j \in {1,\dots, K}$, что ${F_{\bullet i}}^T F_{\bullet j} \gg 0$, а значит эго-графы вершин значительно пересекаются, вершины лежат рядом и велика вероятность того, что принадлежат одному и тому же сообществу.

\subsection{Новый подход.} Решением такой проблемы может послужить дополнительная пенализация за близкое расположение к уже взятым в качестве инициализации вершинам. 
То есть при выборе следующего вектор-столбца $F_{\bullet j}$ матрицы $F$ к значению проводимости $C_i$ эго-графа $i$ вершины добавим штраф, равный 
$$R = \gamma \cdot {F_{\mathrm{selected}}}^T  F_{\mathrm{ego}_i},$$
где $F_{\mathrm{selected}, l} = 1$ в вершинах, которые уже входят в инициализацию ( $\exists k:\, F_{lk}=1 $),
а $F_{\mathrm{ego}_i, l} = 1$, если $l$ лежит в эго-графе $i$ вершины ($l \in \mathcal{N}(i)$), 
$\gamma = 1 / \sum_l F_{\mathrm{selected}, l}$ --- нормировка. 
То есть $R$ --- доля уже выбранных вершин, которые попали в рассматриваемый эго-граф.  

Результат работы метода на текущем примере отражен на правой части рис. \ref{fig:model_ex_graph}. Желаемый результат достигнут. Для большей общности можно было бы ввести коэффициент пенализации, но было решено этого не делать и учитывать 2 критерия (проводимость и коррелированность) с одинаковым весом. 



Таким образом мы избавились от одной описанной выше проблемы. 
Проблема нулей и детерминированности решается простым добавлением равномерного шума в диапазоне $[0; 0,1]$. Константа $0,1$ подобрана экспериментально. 

Дополнительно возникла идея, что вершины соседние к эго-графу имеют большую вероятность принадлежать к тому же сообществу, чем остальные. Значит можно вокруг найденного с помощью проводимости начального приближения "распространить" его на соседние вершины. Т.е. дать половину веса (0.5) всем вершинам, соседним к найденному эго-графу.

\subsection{Эксперименты.} По итогам исследования появилось целое семейство методов инициализации. В ходе экспериментов было изучено поведение правдоподобия на модельных и реальных данных. Т.к. инициализация не основной предмет работы, было решено посмотреть есть ли улучшение качества буквально на нескольких примерах. Более детальное изучение предложенных методов предстоит изучить в дальнейшей работе.

Модельный пример был описан ранее, реальные данные представляют собой набор из 4 небольших эго-графов знакомых автора работы из социальной сети. 

На рис. \ref{fig:llh_init_model} и \ref{fig:llh_init_real} приведено поведение правдоподобия в зависимости от номера итерации для модельных и реальных данных соотвественно. В таблице ниже приведено описание исследуемых методов инициализации:

\note{Жирные линии}\\
\begin{tabular}{ p{9.5em}   p{27em} }
	\hline
	\textbf{Обозначение} & \textbf{Описание} \\
	\hline
	\textit{rand} 				& Инициализация равномерным шумом от 0.75 до 1.25 \\[5px]
	\textit{cond} 				& Инициализация в локальных максимумах проводимости (стандартный метод) \\[5px]
	\textit{cond\_new} 			& Новый метод со штрафом за пересечение с уже выбранными вершинами \\[5px]
	\textit{cond\_randz} 		& Дополнительно заменяем нули из метода *cond* на значения от 0 до 0.1 \\[5px]
	\textit{cond\_new\_randz} 	& Дополнительно заменяем нули из метода *cond\_new* на значения от 0 до 0.1 \\[5px]
	\textit{cond\_randz\_spr} 	& Применяем метод \textit{cond}. Соседние с найденными эго-графами вершины получают половину его веса. Затем заменяем нули матрицы $F$ на значения от 0 до 0.1 \\[5px]
	\textit{cond\_new\_randz\_spr} & Применяем метод \textit{cond\_new}. Соседние с найденными сообществами вершины получают половину его веса. Затем заменяем нули матрицы $F$ на значения от 0 до 0.1 \\[5px]
	\hline
\end{tabular}
\bigskip

\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{imgs/init_llh.png}
	\caption{Поведение минус логарифма правдоподобия в зависимости от номера итерации для различных начальных приближений из таблицы выше. \textbf{Слева:} общий план. \textbf{Справа:} увеличенный участок с 800 итерации. Лучше всех себя покаывает новые методы \textit{cond\_new\_randz} и \textit{cond\_new\_randz\_spr}.}
	\label{fig:llh_init_model}
\end{figure}

\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{imgs/init_llh_real.png}
	\caption{Поведение минус логарифма правдоподобия в зависимости от номера итерации для различных начальных приближений для четырех реальных графов. Показан конец оптимизационной процедуры и 0 по горизонтальной оси соответствует 250 итерации. Во всех четырех случаях лидируют новые методы, только в одном метод без пенализации. }
	\label{fig:llh_init_real}
\end{figure}

\paragraph{Выводы.} Итак, по итогам тестов, можно сказать, что предложенные методы инициализации демонстрируют лучший результат. Методы стартуют из лучших по правдоподобию начальных приближений, сходятся быстрее и к немного лучшим значениям функционала. Если на модельных примерах преимущество было не столь значительным, на реальных данных оно очевидно.

Отметим, что все примеры имели достаточно небольшой размер и маленький диаметр графа, поэтому методы с приставкой \textit{\_spr} проявили себя незначительно, т.к. почти все вершины оказывались соседними к выбранным для инициализации эго-графам. В дальнейшей работе эксперименты будут повторно проведены с большим диаметром графа.

\section{Новые модели для взвешенных графов.}
\note{Наивный переход к взвешенному варианту (деление на вес ребра).\\
	Что-то рассказать про него (?).}

Перейдем к основной теме данной работы: выделение пересекающихся сообществ в взвешенном графе. Такие методы полезны тем, что позволят учесть дополнительную информацию, которая часто дополняет матрицу смежности, но не может напрямую использоваться в методах, работающих с бинарной матрицей смежности, как BigClam.

\note{Нормально написать, структуру дальнейшего текста}
Приведем описание предлагаемых методов.
Затем будут описаны метрики для сравнения получаемых результатов и проведены тесты на модельных и реальных (\note{Убрать, если тесты только на модельных данных}) данных.  

Начнем с самого простого и интуитивного обобщения метода BigClam, который будем называть наивный взвешенный BigClam.
Вес ребра $(u,v)$ обозначим за $w_{uv}$. 
Для обработки взвешенных ребер изменим функционал качества следующим образом:

$$l(F) = \sum_{(u,v)\in E} \log\left(1 - \exp\left( - \dfrac{F_{u} F_{v}^T}{w_{uv}}\right)\right) - \sum_{(u,v) \notin E} F_{u} F_{v}^T.$$

Тем самым получается, что чем больше вес $w_{uv}$, тем больше должно быть значение сил $F_u$ и $F_v$, которые его объясняют, а значит вероятность того, что вершины лежат в одном сообществе повышается.

Т.е. исходное предположение, что вероятность появления ребра $(u,v)$ , при условии, что вершины $u,v$ находятся в одном сообществе $c$ определяется как 

$$P((u,v) \mid c)=1 - \exp\left( -F_{uc} F_{vc} \right),$$

заменяется на предположение, что вероятность появления ребра $(u,v)$ \textit{с весом $w_{uv}$}, при условии, что вершины $u,v$ находятся в одном сообществе $c$ есть 

$$P((u,v) \mid c, w_{uv})=1 - \exp\left(-\dfrac{F_{uc} F_{vc}}{w_{uv}}\right).$$

Однако, такой подход теряет некоторую симметричность, которая была в оригинальной модели. 
Наличие или отсутствие ребра вносят разный вклад в функционал. 
Нельзя предложить похожей красивой вероятностной интерпретации для сил взаимодействия вершин типа $$X_{uv}^{(c)} \sim \mathrm{Pois}\left(\dfrac{F_{uc} \cdot F_{vc}}{w_{uv}}\right),$$
так как в случае отсутствия ребра параметр распределения будет бесконечен.
Если рассматривать 2 случая: наличия, либо отсутствия ребра, получается, что в модели необходимо изначально задать какие ребра присутствуют, а какие нет, иначе из такой модели нельзя будет сгенерировать рассматриваемый граф. Дополнительно в модель изначально заложены ожидаемые веса на ребрах. Все эти наблюдения крайне обременительные. \note{Сверить с научником}.

Простые тесты подтверждают работоспособность модели, и она имеет право на существование.
Описанные сложности в интерпретации такой модели мотивируют искать  другие способы работы с взвешенными данными.

 \note{Добавить прогон на игрушечном примере.}


\subsection{Гамма Модель}
\note{Переход к Гамма моделям.\\
	 Первоначальный вариант. \\
Проблема разреженных данных. \\
Очень долгая сходимость.\\
Переход к Разреженной Гамма модели.\\[20px]}

Попробуем построить новую модель по типу BigClam. Если мы посмотрим на оригинальный метод, то увидим, что все определяется распределением на скрытых переменных $X_{uv}$, которые распределены по Пуассону. Если выпало что угодно кроме нуля --- в графе есть ребро. 

Такая модель легко обобщается на случай целочисленных весов. Просто убираем последнее утверждение, получаем пуассоновкую модель. Для случая непрерывных весов такая модель не подойдет. Значит необходимо подобрать непрерывный аналог распределения Пуассона.  

Самое главное свойство, которые использовались при  выводе оригинального метода — мультипликативность \note{правильный ли термин?} распределения. То есть сумма независимых случайных величин из этого распределения не должна выводить за его пределы.

В качестве непрерывного аналога распределения Пуассона рассмотрим гамма распределение. 
Их сумма (с одинаковым коэффициентом масштаба) не выводит из заданного класса.

Обозначим за $\mathrm{\Gamma}(k, \theta)$ гамма распределение. Аналогично BigClam, выпишем базовые предположения, которые используются в гамма модели.

\paragraph{Предположения}
\begin{enumerate}
	\item Вероятность появления ребра с весом $w^{c}_{uv}$, при условии, что вершины принадлежат сообществу $c$ определяется как
	$$p\left(w^{c}_{uv} \mid c\right) \sim \mathrm{\Gamma}\left(k=F_u F_v^T + 1, \theta=1\right).$$

	\item Каждое сообщество $c$ генерирует ребра независимо друг от друга, а значит, что вес ребра
	$$w_{uv} = \sum_{c} w_{uv}^c \sim \mathrm{\Gamma}\left(\sum_c F_{uc} F_{vc} + 1, 1\right) = \mathrm{\Gamma}\left(F_u F_v^T + 1, 1\right)$$
\end{enumerate}

Поясним почему беремся именно  $F_u F_v^T + 1$, а не $F_u F_v^T$. При $F_u F_v^T=0$ вероятность появления ребра между вершинами должна быть минимальной. $\mathrm{\Gamma}(0, 1)$ является экспоненциальным распределением. Если в графе не существует сообществ, именно это распределение будет объяснять возникающие ребра между вершинами графа, которое является самым естественным для социальных графов.

Для большей наглядности рассуждений везде далее будем опускать параметр $\theta$ и обозначим за $K_{uv} = F_u F_v^T + 1.$ Выведем формулу правдоподобия данной модели.

\paragraph{Модель}
\begin{align*}
l\left(F\right) & = \log\left(\mathbb{P}\left(A\mid F\right)\right) = \sum_{w_{uv}} \log p(w_{uv}) \\
& = \sum_{w_{uv}}\left[-\log\mathrm{\Gamma}\left(K_{uv}\right) - K_{uv}\log\theta + (K_{uv} - 1)\cdot\log w_{uv} - \dfrac{w_{uv}}{\theta}\right] = \big[\theta=1\big]= \\
& = \sum_{w_{uv}}\left[-\log\mathrm{\Gamma}\left(K_{uv}\right) + (K_{uv} - 1)\cdot\log w_{uv} - w_{uv}\right] \\
& = \sum_{w_{uv}}\left[-\log\mathrm{\Gamma}\left( F_u F_v^T + 1 \right) + F_u F_v^T \cdot\log w_{uv} - w_{uv}\right] \rightarrow \max_{F\ge 0}. \\
\end{align*}

\paragraph{Схема оптимизации}

Схема оптимизации используется та же самая, что и в BigCLAM.

Для того, чтобы посчитать градиент, понадобится дигамма функция:

$$\Psi(x) = \dfrac{\mathrm{d}}{\mathrm{d}x} \log\left(\mathrm\Gamma(x)\right).$$

Тогда градиент можем записать как
$$\dfrac{\mathrm{d}l(F)}{\mathrm{d}F_u} = - \sum_v F_v \Psi\left(F_u F_v^T + 1\right) - F_v \log w_{uv}.$$

Ко всем весам прибавляется небольшое $\varepsilon$, чтобы избежать нулей под логарифмом.

По сравнению с оригинальным методом, из-за того, что сумма взвешенная, провести трюк с упрощением сложности вычисления градиента не получится. Для каждого шага, для каждого $F_u$ придется пересчитывать сумму целиком. Получается линейная сложность. Поэтому для подбора шара нецелесообразно использовать backtraking. Используется обычный убывающий шаг.

\paragraph{Эксперименты}
\note{Добавить картинки для экспериментов.}
В ходе экспериментов было рассмотрено 2 варианта моделирования матрицы смежности взвешенного графа. 1 модель была взята прямо из предположений, описанных выше: задается матрица $F$, веса генерируются из гамма распределения. На таких данных оптимизационная схема надежно работает и сходится из любого, даже случайного приближения.

Однако, такая модель данных не соответствует реальным графам, т.к. в настоящие социальные графы разреженные. Вторая модель данных учитывала этот факт. Сначала генерировалась структура графа (есть ребро или нет), затем, только для проявившихся ребер генерируется его вес. Ниже приведен алгоритм генерации:
\begin{enumerate}
	\item Задается матрица $F$ и параметр $\gamma$.
	\item $\forall\, u \in V,v \in V \,$ С вероятностью $1 - \exp(-\gamma F_u {F_v}^T)$ в графе создается ребро $(u, v) \in E$.
	\item $\forall\, (u, v) \in E$ --- созданных ребер генерируется вес $w_{uv} \sim \mathrm{\Gamma}\left(\sum_c F_{uc} F_{vc} + 1, 1\right)$.
\end{enumerate}
Появился дополнительный параметр модели $\gamma$. Чем меньше его значение, тем более разреженной является результирующая матрица смежности $A$.

\paragraph{Анализ.} Оказалось, что предложенная Гамма модель не может объяснить большое количество нулей в подобного рода данных и оптимизация не приводит ни к какому адекватному результату даже из хорошего начального приближения. Необходимо дополнительно учитывать возникающие в данных нули. 

Поэтому было решено отказаться от дальнейшего рассмотрения этой модели и перейти к следующей. Единственное, что стоит отметить, что в данных с малым количеством нулей или полным их отсутствием такой подход может оправдать себя. Например, для решения задачи кластеризации. 

\subsection{Разреженная Гамма модель.}
\note{Введение разреженной составляющей. \\
Вывод формул.\\
 Анализ: Композиция оригинального метода и Гамма модели.\\
Игрушечные примеры.\\}

\paragraph{Предположения.}
Все предположения о природе данных возьмем из описанной в предыдущем параграфе процедуры генерации данных. Собственно, именно эта генеративная модель и привела созданию данного метода.
\paragraph{Модель.}
Обозначим как и раньше вес ребра за $w_{uv}$, а бинаризованную матрицу смежности за $a_{uv}$. То есть $a_{uv} = \mathbb I \left[w_{uv} \ne 0\right]$.

Заметим, что вес ребра $w_{uv}$ отличен от 0 только если $a_{uv}\ne0$, а значит, что 
$$ p(w_{uv}=0 \mid a_{uv}=0) = 1.$$

Теперь, с учетом замечаний, выведем формулу логарифма правдоподобия.
\begin{flalign*}
	l(F) & = \sum_{\forall\,(u,v)} \log p(w_{uv}) = \sum_{u,v} \log p(w_{uv} \mid a_{uv}) + \log p(a_{uv}) \\ 
	& = \sum_{(u,v)\in E} \log p(w_{uv} \mid a_{uv}=1) + \log p(a_{uv}=1) + \sum_{(u,v)\notin E} \log p(w_{uv}=0 \mid a_{uv}=0) + \log p(a_{uv}=0) \\
	& = \sum_{(u,v)\in E} \log p(w_{uv} \mid a_{uv}=1) + \log p(a_{uv}=1) + \sum_{(u,v)\notin E} \log p(a_{uv}=0) \\
	& = \sum_{(u,v)\in E} \log \mathrm{P_\Gamma}(w_{uv}) + \sum_{(u,v)\in E} \log\left(1-\exp\left(-\gamma F_u {F_v}^T\right)\right) - \gamma \sum_{(u,v)\notin E} F_u {F_v}^T.
\end{flalign*}

Первое слагаемое --- это правдоподобие предыдущей гамма модели на ребрах с ненулевыми весами, а последние 2 слагаемых это оригинальная BigClam модель для матрицы $\sqrt \gamma F$! 

Значит, получившаяся модель, является их комбинацией. Модель сохраняет все преимущества BigClam-модели, в том числе скорость вычисления производной, но при этом учитывает взвешенные ребра и имеет дополнительный параметр $\gamma$, который связывает матрицы для гамма и оригинальной модели.

\paragraph{Схема оптимизации.} Поскольку модель является комбинацией двух других, для вычисления градиента необходимо просто сложить градиенты из исходных методов. Отметим только, что в гамма модели сумму необходимо взять не по всем вершинам, а только по соседним к $u$.

Используется схема оптимизации с убывающим шагом, так как backtracking приводил модель к плохим локальным минимумам и оказался неэффективен

\paragraph{Эксперименты.} На аналогичных модельных экспериментах, на которых простая гамма модель не сумела восстановить матрицу F, был запущен новый метод

\note{рисунок}.

Новый метод без труда восстанавливает истинное значение матрицы F, даже если значение параметра $\gamma$ задано не точно.  

\paragraph{Выводы.} Данный метод является главным результатом текущей работы. Метод без существенных затрат обобщает оригинальный bigClam на случай взвешенного графа и имеет под собой понятные и простые вероятностные предположения.

\section{Эксперименты}

\subsection{Функционалы качества}
\note{Про случай пересекающихся сообществ --- немного специфики.}

Оценивать качество получаемых результатов будем по трем функционалам: Модулярности, нормализованной общей информации (Normalized Mutal Information) и среднему значению проводимости (Conductance) сообществ.
Первые два функционала изначально рассматриваются в случае непересекающихся сообществ, поэтому необходимо взять некоторые обобщения предложенных функционалов. Опишем функционалы подробнее.

\paragraph{Модулярность}

Для начала рассмотрим случай непересекающихся сообществ.
Функционал был предложен Ньюманом и Гирваном в ходе разработки алгоритмов кластеризации вершин графа \cite{Girvan02}. 
Модулярность --- это скалярная величина из отрезка $[-1, 1]$, которая определяет качество разбиения на ``модули~~:
$$
Q = \dfrac{1}{2m}\sum_{i,j}\left(A_{ij} - \frac{d_i d_j}{2m}\right) \delta(c_i, c_j),
$$
где $A$ --- Матрица смежности графа, $A_{ij}$ --- $(i,j)$ элемент матрицы,
$d_i$ --- степень $i$ вершины графа,
$c_i$ --- метка вершины (номер сообщества, к которому относится вершина),
$m$ --- общее количество ребер в графе.
$\delta(c_i, c_j)$ --- дельта-функция: равна единице, если $c_i = c_j$, иначе нулю.

Для взвешенных графов, под $A_{ij}$ понимается вес ребра соединяющий вершины $i$ и $j$, а $m = \frac{1}{2}\sum a_{ij}$.

Модулярность по сути сравнивает предложенное разбиение со случайным. Более формально ее значение равно разности между долей ребер внутри сообщества и ожидаемой доли связей, если бы ребра размещены случайно.

Главным недостатком функционала называют его проблема разрешающей способности (функционал плохо работает с маленькими сообществами) \cite{Martelot12}.

Для случая пересекающихся сообществ воспользуемся одним из обобщений, предложенных в \cite{xie2013overlapping}. Возьмем вместо дельта-функции степень принадлежности вершины к сообществу и дополнительно просуммируем по всем сообществам.
$$
Q = \dfrac{1}{2m}\sum_{c\in c}\sum_{i,j}\left(A_{ij} - \frac{d_i d_j}{2m}\right) \beta_{ic} \beta_{jc}\, , 
\quad\text{где}\quad \forall i \quad \sum_c \beta_{ic} = 1.
$$

В случае с рассматриваемыми моделями значение $\beta_{ic}$ легко получить путем нормализации матрицы $F$.

\paragraph{Conductance}
Conductance или проводимость была введена в разделе про инициализацию.
Для того, чтобы использовать предложенную метрику в роли функционала качества разбиения нужно перейти от оценки одного сообщества ко всему разбиению. 
Будем брать среднее и максимальное значение проводимости по всем сообществам. Для того, чтобы использовать значение как метрику качества, вычтем из единицы ее значение.

\paragraph{Normalized Mutal Information}

Опишем метрику в случае непересекающихся сообществ.
Утверждается, что если одно разбиение похоже на другое, то необходимо малое количество информации, чтобы восстановить одно из другого. Значит, значение такой величины можно интерпретировать как меру сходства данных разбиений.

Рассмотрим два разбиения $\{x_i\}$ и $\{y_i\}$, где $i$ это номер вершины, а $x_i$ и $y_i$ метки сообществ из разбиений.
Предполагается, что метки $x$ и $y$ являются значениями двух случайных величин $X$ и $Y$,
которые имеют совместное распределение 
$$P(x,y) = P(X = x, Y = y) = \frac{n_{xy}}{n},$$
где $n$ --- общее количество вершин, $n_{xy}$ - количество вершин, которым в разбиениях $\{x_i\}$ и $\{y_i\}$ сопоставлены метки $x$ и $y$. Аналогично 
$$P(X = x) = \frac{n_x}{n}, \quad
P(Y=y) = \frac{n_y}{n}.$$
Тогда общая информация определяется как
$$
I(X, Y) = H(X) - H(X|Y),
$$
где $H(X)$ --- энтропия распределения $X$, а $H(X|Y)$ --- условная энтропия:

\begin{align*}
	H(X) &= -\sum_{x}P(x)\log P(x),\\
	H(X|Y) &= -\sum_{x,y} P(x,y)\log P(x|y).
\end{align*}

В качестве меры сходства используют нормализованную общую информацию (normalized mutual information):
$$
I_{norm} = \frac{2I(X,Y)}{H(X) + H(Y)}.
$$

Величина $I_{norm}$ равна единице, когда разбиения одинаковые, и нулю, если они независимы. 
Для того чтобы использовать эту величину как меру сходства, достаточно вычитать ее значение из единицы: $1-I_{norm}$.

В случае с пересекающимися сообществами NMI определяется аналогично. В работе используется обобщение предложенное в 
\cite{lancichinetti2009detecting}.

\subsection{Данные}

Большинство стандартных наборов данных, на которых тестируют методы, не подходят для тестирования предложенных методов. Либо нет истинной пересекающейся структуры сообществ, либо граф не взвешенный. Поэтому основные тесты будут проведены на модельном наборе данных.

\paragraph{Модель данных} предложена в \cite{lancichinetti2009benchmarks}. В работе используется код, предоставленный авторами статьи. Модель имеет много параметров. Было выбрано два набора, представленные в таблице \ref{table:bench_params}. Такие значения были выбраны авторами работы \cite{lu2015algorithms} для тестирования своей модели.

Значение параметра $\gamma$ варьируется от 0 до 0.7. В 0 сообщества не пересекаются, при $\gamma=0.7$ сообщества перекрываются на 70\%.
Для анализа построим графики зависимости описанных выше функционалов качества от значения $\gamma$. Чем выше окажется график, тем лучше работает соответствующий метод.

\begin{center}
	\begin{table}
		\note{Жирные линии, во всех таблицах подписи как тут}\\
		\begin{tabular}{ c c l }
			\hline
			\textbf{Параметр} & \textbf{Значение} & \textbf{Описание} \\
			\hline
			$N$				& 1000; 5000 	& Количество вершин	\\[3px]
			$\mu_t$			& 0,1; 0,3 		& Величина смешивания (нечеткость сообществ)\\[3px]
			$k_{\max}$		& 50; 100 		& Максимальная степень вершины\\[3px]
			$k$				& 30; 50 		& Средняя степень вершины\\[3px]
			$\mu_{\omega}$	& 0,1; 0,3 		& Сила смешивания весов на ребрах\\[3px]
			$\gamma$		& от 0 до 0,7 	& Доля вершин в пересекающихся частях сообществ\\[3px]
			$\xi$			& 2 			& Параметр распределения на весах\\[3px]
			$\tau_1$		& 2 			& Параметр распределения на степенях вершин\\[3px]
			$\tau_2$		& 2 			& Параметр распределения на размерах сообществ\\[3px]
			$o_m$			& 2 			& Количество сообществ, \\
							& 				& в которые входит одна вершина при их наложении \\
			\hline
		\end{tabular}
		\caption{Значения параметров в модельных данных}
		\label{table:bench_params}
	\end{table}
\end{center}

\subsection{Результаты экспериментов.}
На рис \ref{fig:experiments_1000} и \ref{fig:experiments_5000} представлены результаты работы методов на модельных данных с 1000 и 5000 вершинами соответственно. Каждая точка является усреднением 20 запусков. Рассматриваются следующие методы.

\begin{enumerate}
	\item \textbf{SparseGamma}			--- Разреженная гамма модель.
	\item \textbf{BigClamWeighted}		--- наивный взвешенный BigClam.
	\item \textbf{BigClam-orig-zeros}	--- оригинальный BigClam.
	\item \textbf{COPRA}				--- label propagation для пересекающегося случая \cite{gregory2010finding}.
	\item \textbf{NMF}					--- Неотрицательное матричное разложение с квадратичной нормой.
\end{enumerate}

\note{Вставить актуальные картинки, поправить список и подписи}
\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{imgs/experiments_1000.png}
	\caption{Результаты работы методов на модельных данных. Граф с 1000 вершинами, параметры указаны в таблице \ref{table:bench_params}. В левом столбце сверху единица минус максимальное значение проводимости, снизу единица минус среднее значение проводимости. В правом столбце сверху NMI, снизу значение модулярности.}
	\label{fig:experiments_1000}
\end{figure}

\begin{figure}[!h]
	\centering
	\includegraphics[width=\textwidth]{imgs/experiments_5000.png}
	\caption{Результаты работы методов на модельных данных. Граф с 5000 вершинами, параметры указаны в таблице \ref{table:bench_params}. В левом столбце сверху единица минус максимальное значение проводимости, снизу единица минус среднее значение проводимости. В правом столбце сверху 1-NMI, снизу значение модулярности.}
	\label{fig:experiments_5000}
\end{figure}

\note{добавить ground truth}

Можно заметить интересную особенность, что такой простой метод как \textit{NMF} в данной задаче имеет подавляющее преимущество по функционалам связанными с проводимостью. При чем, как можно судить по NMI, результирующая структура сообществ получилась хуже чем у любого BigClam метода. Метод \textit{COPRA} с ростом доли вершин в пересекающихся частях сообществ начинает часто в качестве ответа выдавать одно большое сообщество. Это типичная проблема методов типа label propagation.

Интересно, что с ростом параметра $\gamma$ алгоритмы начинают лучше справляться с поставленной задачей.
В случае bigClam методов это можно объяснить. Замечено, что bigClam часто отдает предпочтения структуре сообществ с значительными пересечениями групп вершин между собой. Значит, что чем сильнее пересечение на самом деле, тем проще методу восстановить структуру.  

Что касается двух предложенных метов \textit{SparseGamma} и \textit{BigClamWeighted}, то они ведут себя очень похоже с оригинальным методом \textit{BigCLam}.
Можно предположить, что в данном модельном примере информация, которую с собой несет вес ребра, не достаточно значительна, чтобы при ее удалении ситуация ухудшалась.

\section{Результаты работы}
	
	\paragraph{Выводы.}В работе предложен новый метод для выделения пересекающихся сообществ во взвешенных графах. Метод является обобщением алгоритма BigClam на взвешенный случай.  Метод протестирован на двух модельных наборах данных. Полученные результаты говорят о том, что метод работает на уровне современных методов решающих подобную задачу, но не лучше их. 
	
	В ходе работы предложены новые улучшенные способы инициализации на основе значения проводимости, которые позволяют ускорить существующие методы выделения пересекающихся сообществ.

	\paragraph{Дальнейше исследование.} Из экспериментов видно, что предложенный метод не использует всю ту информацию, которая дополнительно содержится в весах графа. Подход нуждается в дальнейшей доработке. Также будут проведены дополнительные эксперименты по предложенным методам инициализации. 

\newpage

\bibliography{bibl}

\end{document}
